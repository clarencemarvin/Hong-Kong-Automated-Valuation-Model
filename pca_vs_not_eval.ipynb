{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dbb7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.stats import skew\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Tell sklearn to output Pandas DataFrames from transformers\n",
    "# This is crucial for CatBoost's native handling\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c7602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: merged_txn_data.csv\n",
      "Original shape: (5844, 23)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'merged_txn_data.csv'\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded successfully: {file_path}\")\n",
    "    print(f\"Original shape: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at {file_path}\")\n",
    "    # Handle error appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdd1afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after cleaning: (5844, 18)\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering & Cleaning (from your notebook) ---\n",
    "# We use the exact same cleaning steps to ensure a fair comparison.\n",
    "\n",
    "# Drop columns that are irrelevant or have high multicollinearity\n",
    "columns_to_drop = [\n",
    "    'main_district', \n",
    "    'price_per_sqft', \n",
    "    'easting', \n",
    "    'northing', \n",
    "    'distance_to_nearest_mtr_km'\n",
    "]\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Handle outliers\n",
    "def handle_outliers(df, column, threshold=0.01):\n",
    "    low_percentile = df[column].quantile(threshold)\n",
    "    high_percentile = df[column].quantile(1 - threshold)\n",
    "    \n",
    "    # Cap and floor the outliers\n",
    "    df[column] = np.clip(df[column], low_percentile, high_percentile)\n",
    "    return df\n",
    "\n",
    "data = handle_outliers(data, 'saleable_area', threshold=0.01)\n",
    "data = handle_outliers(data, 'property_age', threshold=0.01)\n",
    "\n",
    "# Drop rows with NaN in 'price' (target variable)\n",
    "data.dropna(subset=['price'], inplace=True)\n",
    "\n",
    "print(f\"Data shape after cleaning: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97a49d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature lists defined for Standard and PCA runs.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Feature Lists for A/B Comparison ---\n",
    "\n",
    "# These are the 3 POI features you want to test\n",
    "poi_features_to_test = [\n",
    "    'total_poi_within_1000m',\n",
    "    'category_Education_within_2000m',\n",
    "    'category_Medical_within_2000m'\n",
    "]\n",
    "\n",
    "# These are the *other* numerical features we will always include\n",
    "numerical_base_features = [\n",
    "    'saleable_area',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'bedroom_count',\n",
    "    'property_age',\n",
    "    'travel_time_to_cbd',\n",
    "    'walking_time_to_mtr'\n",
    "]\n",
    "\n",
    "# This is the categorical feature we will always include\n",
    "categorical_features = [\n",
    "    'housing_market_area',\n",
    "]\n",
    "\n",
    "# --- Define feature lists for our TWO experiments ---\n",
    "\n",
    "# 1. Standard Features (No PCA)\n",
    "# We combine the base numerical features + the 3 POI features\n",
    "numerical_cols_standard = numerical_base_features + poi_features_to_test\n",
    "categorical_cols_standard = categorical_features\n",
    "\n",
    "# 2. PCA Features\n",
    "# The list of features for PCA is defined separately\n",
    "# The base numerical features are defined separately\n",
    "# The categorical features are the same\n",
    "numerical_cols_pca_base = numerical_base_features\n",
    "poi_cols_for_pca = poi_features_to_test\n",
    "categorical_cols_pca = categorical_features\n",
    "\n",
    "print(\"Feature lists defined for Standard and PCA runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99747509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training ((4675, 11)) and testing ((1169, 11)) sets.\n"
     ]
    }
   ],
   "source": [
    "# --- Split the Data ---\n",
    "# Use all columns defined so we don't have to split twice\n",
    "# This ensures both runs use the exact same test set\n",
    "all_features = list(set(numerical_cols_standard + categorical_cols_standard))\n",
    "y = data['price']\n",
    "X = data[all_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Data split into training ({X_train.shape}) and testing ({X_test.shape}) sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04391c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Function ---\n",
    "# This function calculates R2, RMSE, MAE, MAPE, and Accuracy Ranges\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100 # Show as percentage\n",
    "    \n",
    "    # Calculate accuracy ranges\n",
    "    error_pct = np.abs((y_test - y_pred) / y_test)\n",
    "    acc_10 = np.mean(error_pct <= 0.1) * 100\n",
    "    acc_20 = np.mean(error_pct <= 0.2) * 100\n",
    "    acc_30 = np.mean(error_pct <= 0.3) * 100\n",
    "    \n",
    "    print(f\"\\n--- Results for {model_name} ---\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:,.2f}\")\n",
    "    print(f\"MAE: {mae:,.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"Accuracy (within 10%): {acc_10:.2f}%\")\n",
    "    print(f\"Accuracy (within 20%): {acc_20:.2f}%\")\n",
    "    print(f\"Accuracy (within 30%): {acc_30:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'R-squared': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'Accuracy (10%)': acc_10,\n",
    "        'Accuracy (20%)': acc_20,\n",
    "        'Accuracy (30%)': acc_30\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddd700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Preprocessors (no PCA) created.\n",
      "PCA Preprocessors created.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Preprocessing Pipelines ---\n",
    "\n",
    "# --- RUN 1: STANDARD PREPROCESSORS (No PCA) ---\n",
    "\n",
    "# 1. Standard Preprocessor (for XGB, LGBM)\n",
    "num_transformer_standard = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_transformer_standard = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor_standard = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer_standard, numerical_cols_standard),\n",
    "        ('cat', cat_transformer_standard, categorical_cols_standard)\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# 2. Standard Preprocessor (for CatBoost)\n",
    "preprocessor_catboost_standard = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer_standard, numerical_cols_standard),\n",
    "        ('cat', 'passthrough', categorical_cols_standard) # <-- Pass-through\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "print(\"Standard Preprocessors (no PCA) created.\")\n",
    "\n",
    "\n",
    "# --- RUN 2: PCA PREPROCESSORS ---\n",
    "\n",
    "# 1. PCA Preprocessor (for XGB, LGBM)\n",
    "# Pipeline for the 3 POI features: Impute -> Scale -> PCA\n",
    "poi_pca_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95, random_state=42)) # Keeps 95% of variance\n",
    "])\n",
    "# Pipeline for the other numerical features: Impute -> Scale\n",
    "num_base_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "# Categorical pipeline is the same\n",
    "cat_pca_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor_pca = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('poi_pca', poi_pca_pipeline, poi_cols_for_pca),\n",
    "        ('num_base', num_base_pipeline, numerical_cols_pca_base),\n",
    "        ('cat', cat_pca_pipeline, categorical_cols_pca)\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# 2. PCA Preprocessor (for CatBoost)\n",
    "preprocessor_catboost_pca = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('poi_pca', poi_pca_pipeline, poi_cols_for_pca),\n",
    "        ('num_base', num_base_pipeline, numerical_cols_pca_base),\n",
    "        ('cat', 'passthrough', categorical_cols_pca) # <-- Pass-through\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "print(\"PCA Preprocessors created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48e0ca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STARTING RUN 1: STANDARD FEATURES\n",
      "==================================================\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "--- Results for XGBoost_Standard ---\n",
      "R-squared: 0.8802\n",
      "RMSE: 1,812,341.29\n",
      "MAE: 1,049,953.50\n",
      "MAPE: 11.20%\n",
      "Accuracy (within 10%): 57.31%\n",
      "Accuracy (within 20%): 85.80%\n",
      "Accuracy (within 30%): 93.76%\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "--- Results for LightGBM_Standard ---\n",
      "R-squared: 0.8766\n",
      "RMSE: 1,839,249.73\n",
      "MAE: 1,061,613.32\n",
      "MAPE: 11.20%\n",
      "Accuracy (within 10%): 58.25%\n",
      "Accuracy (within 20%): 85.12%\n",
      "Accuracy (within 30%): 94.44%\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "--- Results for CatBoost_Standard ---\n",
      "R-squared: 0.8897\n",
      "RMSE: 1,739,045.88\n",
      "MAE: 1,015,752.36\n",
      "MAPE: 10.80%\n",
      "Accuracy (within 10%): 58.60%\n",
      "Accuracy (within 20%): 86.66%\n",
      "Accuracy (within 30%): 94.95%\n"
     ]
    }
   ],
   "source": [
    "# --- Model Training (Run 1 - Standard Features) ---\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"STARTING RUN 1: STANDARD FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_results = [] # To store all metrics\n",
    "\n",
    "# --- XGBoost (Standard) ---\n",
    "pipeline_xgb_std = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_standard),\n",
    "    ('model', xgb.XGBRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "params_xgb = {'model__n_estimators': [500], 'model__learning_rate': [0.1], 'model__max_depth': [8]}\n",
    "grid_xgb_std = GridSearchCV(pipeline_xgb_std, params_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_xgb_std.fit(X_train, y_train)\n",
    "model_results.append(evaluate_model(grid_xgb_std.best_estimator_, X_test, y_test, 'XGBoost_Standard'))\n",
    "\n",
    "# --- LightGBM (Standard) ---\n",
    "pipeline_lgbm_std = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_standard),\n",
    "    ('model', lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1))\n",
    "])\n",
    "params_lgbm = {'model__n_estimators': [500], 'model__learning_rate': [0.1], 'model__num_leaves': [50]}\n",
    "grid_lgbm_std = GridSearchCV(pipeline_lgbm_std, params_lgbm, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_lgbm_std.fit(X_train, y_train)\n",
    "model_results.append(evaluate_model(grid_lgbm_std.best_estimator_, X_test, y_test, 'LightGBM_Standard'))\n",
    "\n",
    "# --- CatBoost (Standard) ---\n",
    "pipeline_cb_std = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_catboost_standard),\n",
    "    ('model', CatBoostRegressor(random_state=42, verbose=0))\n",
    "])\n",
    "params_cb = {'model__iterations': [1000], 'model__learning_rate': [0.1], 'model__depth': [8]}\n",
    "grid_cb_std = GridSearchCV(pipeline_cb_std, params_cb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "# CatBoost needs categorical feature names passed to .fit()\n",
    "X_train_cb = X_train.copy()\n",
    "for col in categorical_cols_standard:\n",
    "    X_train_cb[col] = X_train_cb[col].astype(str)\n",
    "\n",
    "grid_cb_std.fit(X_train_cb, y_train, model__cat_features=categorical_cols_standard)\n",
    "model_results.append(evaluate_model(grid_cb_std.best_estimator_, X_test, y_test, 'CatBoost_Standard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf9e5a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "STARTING RUN 2: PCA FEATURES\n",
      "==================================================\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "--- Results for XGBoost_PCA ---\n",
      "R-squared: 0.8818\n",
      "RMSE: 1,800,332.88\n",
      "MAE: 1,048,704.75\n",
      "MAPE: 11.19%\n",
      "Accuracy (within 10%): 56.80%\n",
      "Accuracy (within 20%): 84.94%\n",
      "Accuracy (within 30%): 94.53%\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "--- Results for LightGBM_PCA ---\n",
      "R-squared: 0.8786\n",
      "RMSE: 1,824,569.45\n",
      "MAE: 1,058,096.02\n",
      "MAPE: 11.33%\n",
      "Accuracy (within 10%): 57.23%\n",
      "Accuracy (within 20%): 85.03%\n",
      "Accuracy (within 30%): 94.10%\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "--- Results for CatBoost_PCA ---\n",
      "R-squared: 0.8870\n",
      "RMSE: 1,760,469.34\n",
      "MAE: 1,036,478.73\n",
      "MAPE: 11.09%\n",
      "Accuracy (within 10%): 57.06%\n",
      "Accuracy (within 20%): 85.71%\n",
      "Accuracy (within 30%): 94.95%\n"
     ]
    }
   ],
   "source": [
    "# --- Model Training (Run 2 - PCA Features) ---\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"STARTING RUN 2: PCA FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# We append to the *same* model_results list\n",
    "\n",
    "# --- XGBoost (PCA) ---\n",
    "pipeline_xgb_pca = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_pca),\n",
    "    ('model', xgb.XGBRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "# We use the *exact same* model parameters\n",
    "grid_xgb_pca = GridSearchCV(pipeline_xgb_pca, params_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_xgb_pca.fit(X_train, y_train)\n",
    "model_results.append(evaluate_model(grid_xgb_pca.best_estimator_, X_test, y_test, 'XGBoost_PCA'))\n",
    "\n",
    "# --- LightGBM (PCA) ---\n",
    "pipeline_lgbm_pca = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_pca),\n",
    "    ('model', lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1))\n",
    "])\n",
    "grid_lgbm_pca = GridSearchCV(pipeline_lgbm_pca, params_lgbm, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_lgbm_pca.fit(X_train, y_train)\n",
    "model_results.append(evaluate_model(grid_lgbm_pca.best_estimator_, X_test, y_test, 'LightGBM_PCA'))\n",
    "\n",
    "# --- CatBoost (PCA) ---\n",
    "pipeline_cb_pca = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_catboost_pca),\n",
    "    ('model', CatBoostRegressor(random_state=42, verbose=0))\n",
    "])\n",
    "grid_cb_pca = GridSearchCV(pipeline_cb_pca, params_cb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "# We use the same prepared X_train_cb from the previous cell\n",
    "grid_cb_pca.fit(X_train_cb, y_train, model__cat_features=categorical_cols_pca)\n",
    "model_results.append(evaluate_model(grid_cb_pca.best_estimator_, X_test, y_test, 'CatBoost_PCA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d80ad475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL MODEL COMPARISON (STANDARD vs. PCA)\n",
      "================================================================================\n",
      "| Model             |   R-squared |         RMSE |          MAE |    MAPE |   Accuracy (10%) |   Accuracy (20%) |   Accuracy (30%) |\n",
      "|:------------------|------------:|-------------:|-------------:|--------:|-----------------:|-----------------:|-----------------:|\n",
      "| CatBoost_Standard |      0.8897 | 1739045.8815 | 1015752.3645 | 10.7956 |          58.5971 |          86.6553 |          94.9530 |\n",
      "| CatBoost_PCA      |      0.8870 | 1760469.3430 | 1036478.7255 | 11.0883 |          57.0573 |          85.7143 |          94.9530 |\n",
      "| XGBoost_PCA       |      0.8818 | 1800332.8819 | 1048704.7500 | 11.1910 |          56.8007 |          84.9444 |          94.5252 |\n",
      "| XGBoost_Standard  |      0.8802 | 1812341.2919 | 1049953.5000 | 11.2026 |          57.3139 |          85.7998 |          93.7553 |\n",
      "| LightGBM_PCA      |      0.8786 | 1824569.4467 | 1058096.0154 | 11.3348 |          57.2284 |          85.0299 |          94.0975 |\n",
      "| LightGBM_Standard |      0.8766 | 1839249.7286 | 1061613.3225 | 11.2041 |          58.2549 |          85.1155 |          94.4397 |\n",
      "\n",
      "Comparison table saved to 'model_comparison_standard_vs_pca.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Final Comparison ---\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Sort by R-squared to see the best performing model\n",
    "results_df = results_df.sort_values(by='R-squared', ascending=False)\n",
    "\n",
    "# Display the final comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON (STANDARD vs. PCA)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_markdown(index=False, floatfmt=\".4f\"))\n",
    "\n",
    "# Save the comparison to a CSV file\n",
    "results_df.to_csv(\"model_comparison_standard_vs_pca.csv\", index=False)\n",
    "print(\"\\nComparison table saved to 'model_comparison_standard_vs_pca.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3522",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
